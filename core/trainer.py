import os
import torch
import json
from transformers import TrainerCallback
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset


class LossLoggerCallback(TrainerCallback):
    """å®æ—¶è®°å½•lossåˆ°JSONæ–‡ä»¶"""

    def __init__(self, output_file):
        self.output_file = output_file
        self.loss_history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            log_entry = {
                "step": state.global_step,
                "epoch": state.epoch,
                "loss": logs.get("loss"),
                "learning_rate": logs.get("learning_rate"),
            }
            self.loss_history.append(log_entry)

            # å®æ—¶ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.output_file, 'w') as f:
                json.dump({"log_history": self.loss_history}, f, indent=2)


def train_model(
        task_id,
        data_path,
        user_args,
):
    base_model = user_args.get("base_model", "Qwen/Qwen2.5-0.5B-Instruct")
    # ================= 1. è·¯å¾„è®¾ç½® =================
    current_file_path = os.path.abspath(__file__)
    core_dir = os.path.dirname(current_file_path)
    project_root = os.path.dirname(core_dir)
    output_dir = os.path.join(project_root, "output", task_id)
    logging_dir = os.path.join(project_root, "logs", task_id)

    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(logging_dir, exist_ok=True)

    print(f"\n[EasyTune] ğŸš€ ä»»åŠ¡å¯åŠ¨: {task_id}")

    # ================= 2. åŠ è½½ Tokenizer =================
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    # ä¿®å¤ Warning çš„å…³é”®ï¼šå¦‚æœæ²¡æœ‰ pad_tokenï¼Œå°±ç”¨ eos_token é¡¶æ›¿
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ================= 3. æ•°æ®å¤„ç† (æ ¸å¿ƒä¿®æ­£!!!) =================
    def process_func(example):
        instruction = example.get('instruction', '')
        response = example.get('output', '')

        # --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ---
        # ä½¿ç”¨ apply_chat_template ä¿è¯å’Œæ¨ç†æ—¶çš„æ ¼å¼ä¸€æ¨¡ä¸€æ ·
        messages = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": response}
        ]
        # ç”Ÿæˆæ ‡å‡†çš„è®­ç»ƒæ–‡æœ¬
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

        ids = tokenizer(text, padding=False, truncation=True, max_length=512)
        return {
            "input_ids": ids["input_ids"],
            "attention_mask": ids["attention_mask"],
            "labels": ids["input_ids"]
        }

    dataset = load_dataset("json", data_files=data_path, split="train")
    tokenized_ds = dataset.map(process_func)

    # ================= 4. åŠ è½½æ¨¡å‹ =================
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        r=user_args.get("lora_r", 8),
        lora_alpha=user_args.get("lora_alpha", 16),
        lora_dropout=0.1
    )
    model = get_peft_model(model, peft_config)

    # ================= 5. è®­ç»ƒå‚æ•° (åŠ å¼ºç‰ˆ) =================
    args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=user_args.get("batch_size", 2),
        gradient_accumulation_steps=user_args.get("gradient_accumulation_steps", 4),
        num_train_epochs=user_args.get("epoch", 1),
        learning_rate=user_args.get("learning_rate", 3e-4),
        logging_steps=1,
        logging_dir=str(logging_dir),
        save_strategy="epoch",
        fp16=torch.cuda.is_available(),
        use_cpu=not torch.cuda.is_available(),
        report_to=None
    )

    loss_callback = LossLoggerCallback(os.path.join(logging_dir, "trainer_state.json"))

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_ds,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
        callbacks=[loss_callback]  # æ–°å¢ï¼šæ·»åŠ è‡ªå®šä¹‰å›è°ƒ
    )

    print("[EasyTune] â–¶ï¸  å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    model.save_pretrained(output_dir)
    print(f"[EasyTune] âœ… è®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    return output_dir


if __name__ == "__main__":
    train_model(
        task_id="ewew",
        data_path="../data/45d8e227-8808-4230-a16c-f6be7296d4d5.json",
        user_args = {
        }
    )
