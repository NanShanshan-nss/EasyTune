import sys
import os
# å¼ºåˆ¶æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from fastapi import FastAPI, BackgroundTasks, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import shutil
import uuid
import sqlite3
import json
import torch
import pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from core.trainer import train_model

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- æ•°æ®åº“åˆå§‹åŒ– ---
DB_PATH = "tasks.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
            task_id TEXT PRIMARY KEY,
            status TEXT,
            file_path TEXT,
            args TEXT,
            output_path TEXT,
            error TEXT
        )
    ''')
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»ï¼ˆæ·»åŠ æ–°åˆ—ï¼‰
    cursor.execute("PRAGMA table_info(tasks)")
    columns = [info[1] for info in cursor.fetchall()]
    
    if 'args' not in columns:
        cursor.execute("ALTER TABLE tasks ADD COLUMN args TEXT")
    if 'output_path' not in columns:
        cursor.execute("ALTER TABLE tasks ADD COLUMN output_path TEXT")
        
    conn.commit()
    conn.close()

init_db()

# --- ç³»ç»Ÿç›‘æ§ ---
def get_gpu_status():
    status = {}
    
    # è·å– GPU (NVIDIA)
    try:
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        if device_count > 0:
            # é»˜è®¤åªçœ‹ç¬¬ä¸€å¼ å¡
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)
            
            name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(name, bytes):
                name = name.decode("utf-8")

            status.update({
                "gpu_name": name,
                "gpu_memory_used": mem_info.used // 1024**2,
                "gpu_memory_total": mem_info.total // 1024**2,
                "gpu_util": util_info.gpu,
                "gpu_memory_util": util_info.memory
            })
        else:
            status["gpu_error"] = "No NVIDIA GPU found"
        
        pynvml.nvmlShutdown()
        
    except Exception as e:
        status["gpu_error"] = str(e)
        # å¡«å……é»˜è®¤å€¼é˜²æ­¢å‰ç«¯æŠ¥é”™
        status.update({
            "gpu_memory_used": 0,
            "gpu_memory_total": 0,
            "gpu_util": 0
        })

    return status

@app.get("/system_status")
async def system_status():
    return get_gpu_status()

# --- å…¨å±€æ¨ç†å¼•æ“ (å•ä¾‹æ¨¡å¼) ---
class InferenceEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"
        self.current_lora_path = None
    
    def load_base_model(self):
        if self.model is None:
            print("â³ [Server] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name, 
                device_map="auto", 
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            print("âœ… [Server] åŸºåº§æ¨¡å‹åŠ è½½å®Œæ¯•")

    def get_response(self, query: str, lora_path: str = None):
        self.load_base_model() # ç¡®ä¿åŸºåº§å·²åŠ è½½
        
        # æƒ…å†µ 1: ç”¨æˆ·æƒ³ç”¨ LoRA å¾®è°ƒæ¨¡å‹
        if lora_path:
            # å¦‚æœå½“å‰æŒ‚è½½çš„ä¸æ˜¯è¿™ä¸ª LoRAï¼Œæˆ–è€…å½“å‰æ˜¯çº¯åŸºåº§ï¼Œå°±éœ€è¦åˆ‡æ¢
            if self.current_lora_path != lora_path:
                print(f"ğŸ”„ [Server] åˆ‡æ¢åˆ°å¾®è°ƒæ¨¡å‹: {lora_path}")
                try:
                    # 1. ä¸ºäº†é˜²æ­¢æ˜¾å­˜æ³„éœ²æˆ–å†²çªï¼Œå…ˆå¼ºåˆ¶é‡æ–°åŠ è½½ä¸€éçº¯å‡€çš„åŸºåº§
                    # (è™½ç„¶ç¨å¾®æ…¢ç‚¹ï¼Œä½†ç»å¯¹ç¨³ï¼Œä¸ä¼šæŠ¥é”™)
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.base_model_name, 
                        device_map="auto", 
                        trust_remote_code=True,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                    )
                    
                    # 2. æŒ‚è½½ LoRA
                    self.model = PeftModel.from_pretrained(self.model, lora_path)
                    self.current_lora_path = lora_path
                    
                except Exception as e:
                    return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        
        # æƒ…å†µ 2: ç”¨æˆ·æƒ³ç”¨çº¯åŸºåº§æ¨¡å‹
        else:
            # å¦‚æœå½“å‰æŒ‚ç€ LoRAï¼Œè¯´æ˜éœ€è¦å¸è½½
            if self.current_lora_path is not None:
                print("ğŸ”„ [Server] åˆ‡æ¢å›åŸºåº§æ¨¡å‹ (å¸è½½ LoRA)")
                try:
                    # ä¿®æ­£ï¼šä¸è¦ç”¨ unload_adapterï¼Œç›´æ¥é‡è½½åŸºåº§æœ€ç¨³å¦¥
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.base_model_name, 
                        device_map="auto", 
                        trust_remote_code=True,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                    )
                    self.current_lora_path = None
                except Exception as e:
                    return f"âŒ åˆ‡æ¢åŸºåº§å¤±è´¥: {str(e)}"

        # --- å¼€å§‹ç”Ÿæˆ ---
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": query}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            generated_ids = self.model.generate(
                inputs.input_ids,
                max_new_tokens=256,
                temperature=0.7
            )
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine()

# --- API å®šä¹‰ ---
class TrainRequest(BaseModel):
    file_id: str
    args: dict = None

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    file_id = str(uuid.uuid4())
    # ç¡®ä¿ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ data æ–‡ä»¶å¤¹
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
    os.makedirs(data_dir, exist_ok=True)
    file_path = os.path.join(data_dir, f"{file_id}.json")
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    return {"filename": file.filename, "file_id": file_id}

@app.post("/train")
async def start_training(req: TrainRequest,
                         background_tasks: BackgroundTasks):
    task_id = str(uuid.uuid4())
    # é‡æ–°æ„å»º data è·¯å¾„
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
    data_path = os.path.join(data_dir, f"{req.file_id}.json")
    
    # é¢„è®¡ç®— output_path
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, "output", task_id)

    # å†™å…¥æ•°æ®åº“
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    args_json = json.dumps(req.args) if req.args else "{}"
    cursor.execute(
        "INSERT INTO tasks (task_id, status, file_path, args, output_path) VALUES (?, ?, ?, ?, ?)", 
        (task_id, "running", data_path, args_json, output_path)
    )
    conn.commit()
    conn.close()

    background_tasks.add_task(run_training_background, task_id, data_path, req.args)
    return {"task_id": task_id, "status": "started"}

def run_training_background(task_id, data_path, user_args):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    try:
        train_model(task_id, data_path, user_args)
        cursor.execute("UPDATE tasks SET status = ? WHERE task_id = ?", ("success", task_id))
    except Exception as e:
        cursor.execute("UPDATE tasks SET status = ?, error = ? WHERE task_id = ?", ("failed", str(e), task_id))
    finally:
        conn.commit()
        conn.close()

@app.get("/status/{task_id}")
async def get_status(task_id: str):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT status, error, file_path, args, output_path FROM tasks WHERE task_id = ?", (task_id,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        status, error, file_path, args, output_path = row
        response = {
            "task_id": task_id,
            "status": status,
            "file_path": file_path,
            "args": json.loads(args) if args else None,
            "output_path": output_path
        }
        if error:
            response["error"] = error
        return response
    else:
        return {"status": "not_found"}

@app.get("/tasks")
async def list_tasks():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT task_id, status, file_path, args, output_path, error FROM tasks")
    rows = cursor.fetchall()
    conn.close()
    
    tasks_list = []
    for row in rows:
        task_id, status, file_path, args, output_path, error = row
        tasks_list.append({
            "task_id": task_id,
            "status": status,
            "file_path": file_path,
            "args": json.loads(args) if args else None,
            "output_path": output_path,
            "error": error
        })
    return tasks_list

# æ–°å¢ï¼šèŠå¤©æ¥å£
class ChatRequest(BaseModel):
    query: str
    task_id: str = None   # å¦‚æœä¸ºç©ºï¼Œå°±æ˜¯åŸºåº§ï¼›å¦‚æœæœ‰å€¼ï¼Œå°±æ˜¯å¾®è°ƒ
    use_lora: bool = False

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    lora_path = None
    if req.use_lora and req.task_id:
        # æ„å»º LoRA çš„ç»å¯¹è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        lora_path = os.path.join(project_root, "output", req.task_id)
        if not os.path.exists(lora_path):
            return {"response": "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¯¥ä»»åŠ¡çš„è®­ç»ƒç»“æœï¼Œè¯·æ£€æŸ¥Task ID"}
    
    response = engine.get_response(req.query, lora_path)
    return {"response": response}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)