import sys
import os
# å¼ºåˆ¶æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from fastapi import FastAPI, BackgroundTasks, UploadFile, File, HTTPException
from pydantic import BaseModel
import shutil
import uuid
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from core.trainer import train_model

app = FastAPI()

# --- å…¨å±€æ¨ç†å¼•æ“ (å•ä¾‹æ¨¡å¼) ---
class InferenceEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"
        self.current_lora_path = None
    
    def load_base_model(self):
        if self.model is None:
            print("â³ [Server] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name, 
                device_map="auto", 
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            print("âœ… [Server] åŸºåº§æ¨¡å‹åŠ è½½å®Œæ¯•")

    def get_response(self, query: str, lora_path: str = None):
        self.load_base_model() # ç¡®ä¿åŸºåº§å·²åŠ è½½
        
        # æƒ…å†µ 1: ç”¨æˆ·æƒ³ç”¨ LoRA å¾®è°ƒæ¨¡å‹
        if lora_path:
            # å¦‚æœå½“å‰æŒ‚è½½çš„ä¸æ˜¯è¿™ä¸ª LoRAï¼Œæˆ–è€…å½“å‰æ˜¯çº¯åŸºåº§ï¼Œå°±éœ€è¦åˆ‡æ¢
            if self.current_lora_path != lora_path:
                print(f"ğŸ”„ [Server] åˆ‡æ¢åˆ°å¾®è°ƒæ¨¡å‹: {lora_path}")
                try:
                    # 1. ä¸ºäº†é˜²æ­¢æ˜¾å­˜æ³„éœ²æˆ–å†²çªï¼Œå…ˆå¼ºåˆ¶é‡æ–°åŠ è½½ä¸€éçº¯å‡€çš„åŸºåº§
                    # (è™½ç„¶ç¨å¾®æ…¢ç‚¹ï¼Œä½†ç»å¯¹ç¨³ï¼Œä¸ä¼šæŠ¥é”™)
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.base_model_name, 
                        device_map="auto", 
                        trust_remote_code=True,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                    )
                    
                    # 2. æŒ‚è½½ LoRA
                    self.model = PeftModel.from_pretrained(self.model, lora_path)
                    self.current_lora_path = lora_path
                    
                except Exception as e:
                    return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        
        # æƒ…å†µ 2: ç”¨æˆ·æƒ³ç”¨çº¯åŸºåº§æ¨¡å‹
        else:
            # å¦‚æœå½“å‰æŒ‚ç€ LoRAï¼Œè¯´æ˜éœ€è¦å¸è½½
            if self.current_lora_path is not None:
                print("ğŸ”„ [Server] åˆ‡æ¢å›åŸºåº§æ¨¡å‹ (å¸è½½ LoRA)")
                try:
                    # ä¿®æ­£ï¼šä¸è¦ç”¨ unload_adapterï¼Œç›´æ¥é‡è½½åŸºåº§æœ€ç¨³å¦¥
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.base_model_name, 
                        device_map="auto", 
                        trust_remote_code=True,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                    )
                    self.current_lora_path = None
                except Exception as e:
                    return f"âŒ åˆ‡æ¢åŸºåº§å¤±è´¥: {str(e)}"

        # --- å¼€å§‹ç”Ÿæˆ ---
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": query}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            generated_ids = self.model.generate(
                inputs.input_ids,
                max_new_tokens=256,
                temperature=0.7
            )
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

# åˆå§‹åŒ–å¼•æ“
engine = InferenceEngine()

# --- API å®šä¹‰ ---
tasks = {}

class TrainRequest(BaseModel):
    file_id: str
    args: dict = None

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    file_id = str(uuid.uuid4())
    # ç¡®ä¿ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ data æ–‡ä»¶å¤¹
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
    os.makedirs(data_dir, exist_ok=True)
    file_path = os.path.join(data_dir, f"{file_id}.json")
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    return {"filename": file.filename, "file_id": file_id}

@app.post("/train")
async def start_training(req: TrainRequest,
                         background_tasks: BackgroundTasks):
    task_id = str(uuid.uuid4())
    # é‡æ–°æ„å»º data è·¯å¾„
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
    data_path = os.path.join(data_dir, f"{req.file_id}.json")
    
    tasks[task_id] = {"status": "running"}
    background_tasks.add_task(run_training_background, task_id, data_path, req.args)
    return {"task_id": task_id, "status": "started"}

def run_training_background(task_id, data_path, user_args):
    try:
        train_model(task_id, data_path,user_args)
        tasks[task_id]["status"] = "success"
    except Exception as e:
        tasks[task_id]["status"] = "failed"
        tasks[task_id]["error"] = str(e)

@app.get("/status/{task_id}")
async def get_status(task_id: str):
    return tasks.get(task_id, {"status": "not_found"})

# æ–°å¢ï¼šèŠå¤©æ¥å£
class ChatRequest(BaseModel):
    query: str
    task_id: str = None   # å¦‚æœä¸ºç©ºï¼Œå°±æ˜¯åŸºåº§ï¼›å¦‚æœæœ‰å€¼ï¼Œå°±æ˜¯å¾®è°ƒ
    use_lora: bool = False

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    lora_path = None
    if req.use_lora and req.task_id:
        # æ„å»º LoRA çš„ç»å¯¹è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        lora_path = os.path.join(project_root, "output", req.task_id)
        if not os.path.exists(lora_path):
            return {"response": "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¯¥ä»»åŠ¡çš„è®­ç»ƒç»“æœï¼Œè¯·æ£€æŸ¥Task ID"}
    
    response = engine.get_response(req.query, lora_path)
    return {"response": response}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)