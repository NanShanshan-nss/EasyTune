import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# ---------------- é…ç½®åŒºåŸŸ ----------------
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_ROOT = "./output"
# ----------------------------------------

def get_latest_lora_path():
    """è‡ªåŠ¨å¯»æ‰¾ output æ–‡ä»¶å¤¹é‡Œæœ€æ–°çš„é‚£ä¸ªæ¨¡å‹"""
    if not os.path.exists(OUTPUT_ROOT):
        return None
    
    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    all_subdirs = [os.path.join(OUTPUT_ROOT, d) for d in os.listdir(OUTPUT_ROOT) if os.path.isdir(os.path.join(OUTPUT_ROOT, d))]
    
    if not all_subdirs:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ‰¾æœ€æ–°çš„
    latest_subdir = max(all_subdirs, key=os.path.getmtime)
    return latest_subdir

def chat_with_model():
    print("ğŸ” æ­£åœ¨è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„è®­ç»ƒæ¨¡å‹...")
    lora_path = get_latest_lora_path()
    
    if lora_path:
        print(f"âœ… æ‰¾åˆ°äº†æœ€æ–°æ¨¡å‹è·¯å¾„: {lora_path}")
    else:
        print("âŒ åœ¨ output æ–‡ä»¶å¤¹é‡Œæ²¡æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼è¯·å…ˆå»ç½‘é¡µä¸Šè®­ç»ƒä¸€ä¸ªä»»åŠ¡ã€‚")
        return

    print("â³ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ (Qwen2.5-0.5B)...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
        # è‡ªåŠ¨æ£€æµ‹æ˜¾å¡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½åŸºåº§
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL, 
            device_map="auto", 
            trust_remote_code=True,
            torch_dtype=torch.float16 if device=="cuda" else torch.float32
        )
    except Exception as e:
        print(f"âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚æŠ¥é”™: {e}")
        return
    
    print(f"â³ æ­£åœ¨æŒ‚è½½ LoRA è¡¥ä¸...")
    try:
        model = PeftModel.from_pretrained(model, lora_path)
        print("ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ LoRAåŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥ {lora_path} ä¸‹æ˜¯å¦æœ‰ adapter_config.jsonã€‚æŠ¥é”™: {e}")
        return

    print("\n" + "="*30)
    print("ğŸ¤– EasyTune å¯¹è¯ç»ˆç«¯ (è¾“å…¥ quit é€€å‡º)")
    print("="*30)
    
    # ç®€å•çš„å¯¹è¯å†å²ï¼Œè®©å®ƒèƒ½è®°ä½ä¸Šä¸‹æ–‡
    history = [] 

    while True:
        query = input("\nğŸ‘¤ ç”¨æˆ·: ")
        if query.strip().lower() == "quit":
            break
            
        # æ„å»º Prompt
        messages = [
            {"role": "system", "content": "You are a helpful assistant."}
        ]
        # ç®€å•çš„å¤šè½®å¯¹è¯æ‹¼æ¥ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        for h in history:
            messages.append({"role": "user", "content": h[0]})
            messages.append({"role": "assistant", "content": h[1]})
        
        messages.append({"role": "user", "content": query})
        
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(device)

        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=200, # å›å¤æœ€å¤§é•¿åº¦
                temperature=0.7     # æ§åˆ¶åˆ›é€ æ€§
            )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print(f"ğŸ¤– EasyTune: {response}")
        
        # è®°å½•å†å²
        history.append((query, response))
        if len(history) > 3: history.pop(0) # åªè®°æœ€è¿‘3è½®

if __name__ == "__main__":
    chat_with_model()