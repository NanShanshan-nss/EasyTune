import os
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

def train_model(task_id, data_path, base_model="Qwen/Qwen2.5-0.5B-Instruct", epoch=10):
    # ================= 1. è·¯å¾„è®¾ç½® =================
    current_file_path = os.path.abspath(__file__)
    core_dir = os.path.dirname(current_file_path)
    project_root = os.path.dirname(core_dir)
    output_dir = os.path.join(project_root, "output", task_id)
    logging_dir = os.path.join(project_root, "logs", task_id)
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(logging_dir, exist_ok=True)
    
    print(f"\n[EasyTune] ğŸš€ ä»»åŠ¡å¯åŠ¨: {task_id}")

    # ================= 2. åŠ è½½ Tokenizer =================
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    # ä¿®å¤ Warning çš„å…³é”®ï¼šå¦‚æœæ²¡æœ‰ pad_tokenï¼Œå°±ç”¨ eos_token é¡¶æ›¿
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ================= 3. æ•°æ®å¤„ç† (æ ¸å¿ƒä¿®æ­£!!!) =================
    def process_func(example):
        instruction = example.get('instruction', '')
        response = example.get('output', '')
        
        # --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ---
        # ä½¿ç”¨ apply_chat_template ä¿è¯å’Œæ¨ç†æ—¶çš„æ ¼å¼ä¸€æ¨¡ä¸€æ ·
        messages = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": response}
        ]
        # ç”Ÿæˆæ ‡å‡†çš„è®­ç»ƒæ–‡æœ¬
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---
        
        ids = tokenizer(text, padding=False, truncation=True, max_length=512)
        return {
            "input_ids": ids["input_ids"], 
            "attention_mask": ids["attention_mask"], 
            "labels": ids["input_ids"]
        }

    dataset = load_dataset("json", data_files=data_path, split="train")
    tokenized_ds = dataset.map(process_func)

    # ================= 4. åŠ è½½æ¨¡å‹ =================
    model = AutoModelForCausalLM.from_pretrained(
        base_model, 
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, 
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        r=8, 
        lora_alpha=32, 
        lora_dropout=0.1
    )
    model = get_peft_model(model, peft_config)

    # ================= 5. è®­ç»ƒå‚æ•° (åŠ å¼ºç‰ˆ) =================
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        num_train_epochs=epoch,      # å»ºè®®åœ¨ç½‘é¡µä¸ŠæŠŠè¿™ä¸ªæ•°å­—è®¾å¤§ä¸€ç‚¹
        learning_rate=3e-4,          #ç¨å¾®è°ƒå¤§ä¸€ç‚¹å­¦ä¹ ç‡ï¼Œè®©å®ƒå­¦å¾—æ›´å¿«
        logging_dir=logging_dir,
        logging_steps=5,
        save_strategy="epoch",
        save_total_limit=1,
        fp16=torch.cuda.is_available(),
        use_cpu=not torch.cuda.is_available(),
        report_to=None
    )
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_ds,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    )
    
    print("[EasyTune] â–¶ï¸  å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    model.save_pretrained(output_dir)
    print(f"[EasyTune] âœ… è®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    return output_dir